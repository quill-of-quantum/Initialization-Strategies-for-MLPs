{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7253582",
   "metadata": {},
   "source": [
    "### Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020162e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "CONFIG = {\n",
    "    \"layers_list\": [5, 10, 20, 30, 40, 50],\n",
    "    \"init_types\": [\n",
    "        \"he_normal\", \"he_uniform\",\n",
    "        \"xavier_normal\", \"xavier_uniform\",\n",
    "        \"normal\", \"zero\",\n",
    "        \"orthogonal\", \"trunc_normal\"\n",
    "    ],\n",
    "    \"activations\": [\"relu\", \"tanh\", \"sigmoid\", \"gelu\"],\n",
    "    \"init_params\": {\n",
    "        \"he_normal\":   {\"factor\": 2.0, \"mode\": \"fan_in\", \"nonlinearity\": \"relu\"},\n",
    "        \"he_uniform\":  {\"factor\": 2.0, \"mode\": \"fan_in\", \"nonlinearity\": \"relu\"},\n",
    "        \"orthogonal\":  {\"gain\": 1.0},\n",
    "        \"trunc_normal\": {\"mean\": 0.0, \"std\": 0.01, \"a\": -2.0, \"b\": 2.0}\n",
    "    },\n",
    "    \"hidden_size\": 128,\n",
    "    \"input_size\": 784,\n",
    "    \"output_size\": 10,\n",
    "    \"num_epochs\": 50,\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"save_dir\": \"results\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d423ac",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56053202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from typing import Dict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed=42):  # Set random seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e20833",
   "metadata": {},
   "source": [
    "### Create result output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c45f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result output directory\n",
    "def make_plot_dir():\n",
    "    os.makedirs(CONFIG[\"save_dir\"], exist_ok=True)\n",
    "\n",
    "# Activation function selector\n",
    "def get_activation(name: str):\n",
    "    if name == \"relu\":    return F.relu\n",
    "    if name == \"tanh\":    return torch.tanh\n",
    "    if name == \"sigmoid\": return torch.sigmoid\n",
    "    if name == \"gelu\":    return F.gelu\n",
    "    raise ValueError(f\"Unsupported activation: {name}\")\n",
    "\n",
    "# Model definition\n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_hidden_layers, activation):\n",
    "        super().__init__()\n",
    "        self.activation_fn = get_activation(activation)\n",
    "        self.hidden = nn.ModuleList([\n",
    "            nn.Linear(input_size if i == 0 else hidden_size, hidden_size)\n",
    "            for i in range(num_hidden_layers)\n",
    "        ])\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden:\n",
    "            x = self.activation_fn(layer(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# Weight initialization\n",
    "def initialize_weights(model: nn.Module, init_type: str, params: Dict = None):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            p = params or {}\n",
    "            if init_type == 'he_normal':\n",
    "                fan = nn.init._calculate_correct_fan(m.weight, p.get('mode','fan_in'))\n",
    "                std = math.sqrt(p.get('factor',2.0) / fan)\n",
    "                nn.init.normal_(m.weight, 0.0, std)\n",
    "            elif init_type == 'he_uniform':\n",
    "                fan = nn.init._calculate_correct_fan(m.weight, p.get('mode','fan_in'))\n",
    "                bound = math.sqrt(3.0 * p.get('factor',2.0) / fan)\n",
    "                nn.init.uniform_(m.weight, -bound, bound)\n",
    "            elif init_type == 'xavier_normal': nn.init.xavier_normal_(m.weight)\n",
    "            elif init_type == 'xavier_uniform': nn.init.xavier_uniform_(m.weight)\n",
    "            elif init_type == 'normal': nn.init.normal_(m.weight, p.get('mean',0.0), p.get('std',0.1))\n",
    "            elif init_type == 'zero': nn.init.constant_(m.weight, 0.0)\n",
    "            elif init_type == 'orthogonal': nn.init.orthogonal_(m.weight, p.get('gain',1.0))\n",
    "            elif init_type == 'trunc_normal':\n",
    "                nn.init.trunc_normal_(m.weight, p.get('mean',0.0), p.get('std',0.1), p.get('a',-2.0), p.get('b',2.0))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown init type: {init_type}\")\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# Data loading\n",
    "def get_dataloaders(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST('./data', True, download=True, transform=transform)\n",
    "    test_dataset  = datasets.MNIST('./data', False, download=True, transform=transform)\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training step\n",
    "def train(model, optimizer, criterion, loader, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (output.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "# Evaluation step\n",
    "def evaluate(model, criterion, loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            correct += (output.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "# Experiment execution (includes activation & gradient hooks + stats saving)\n",
    "def run_experiment(init_type: str, num_layers: int, activation: str, config: Dict):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_loader, test_loader = get_dataloaders(config['batch_size'])\n",
    "    model = DeepMLP(config['input_size'], config['hidden_size'], config['output_size'], num_layers, activation).to(device)\n",
    "\n",
    "    # Hook containers\n",
    "    layer_acts = [[] for _ in model.hidden]\n",
    "    layer_grads = [[] for _ in model.hidden]\n",
    "\n",
    "    # Hook factory functions\n",
    "    def make_forward_hook(idx):\n",
    "        def forward_hook(module, inp, out):\n",
    "            layer_acts[idx].append(out.detach().cpu().numpy())\n",
    "        return forward_hook\n",
    "    def make_backward_hook(idx):\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            layer_grads[idx].append(grad_out[0].detach().cpu().numpy())\n",
    "        return backward_hook\n",
    "\n",
    "    # Register hooks\n",
    "    for idx, layer in enumerate(model.hidden):\n",
    "        layer.register_forward_hook(make_forward_hook(idx))\n",
    "        layer.register_full_backward_hook(make_backward_hook(idx))\n",
    "\n",
    "    # Initialize weights\n",
    "    init_params = config.get('init_params', {}).get(init_type, {})\n",
    "    initialize_weights(model, init_type, init_params)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Build training history dict (with basic and per-layer stats)\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': [], 'epoch_time': []}\n",
    "    for i in range(num_layers):\n",
    "        history[f'act_mean_{i}'] = []\n",
    "        history[f'act_std_{i}'] = []\n",
    "        history[f'grad_norm_{i}'] = []\n",
    "\n",
    "    prefix = f\"{init_type}_{activation}_{num_layers}L\"\n",
    "\n",
    "    # Epoch loop\n",
    "    for ep in range(1, config['num_epochs'] + 1):\n",
    "        # Clear caches\n",
    "        for lst in layer_acts: lst.clear()\n",
    "        for lst in layer_grads: lst.clear()\n",
    "\n",
    "        start = time.time()\n",
    "        tl, ta = train(model, optimizer, criterion, train_loader, device)\n",
    "        vl, va = evaluate(model, criterion, test_loader, device)\n",
    "        duration = time.time() - start\n",
    "\n",
    "        # Record basic metrics\n",
    "        history['train_loss'].append(tl)\n",
    "        history['test_loss'].append(vl)\n",
    "        history['train_acc'].append(ta)\n",
    "        history['test_acc'].append(va)\n",
    "        history['epoch_time'].append(duration)\n",
    "\n",
    "        # Record per-layer stats\n",
    "        for i, layer in enumerate(model.hidden):\n",
    "            acts_arr = np.concatenate([a.reshape(-1) for a in layer_acts[i]]) if layer_acts[i] else np.array([])\n",
    "            history[f'act_mean_{i}'].append(acts_arr.mean() if acts_arr.size else np.nan)\n",
    "            history[f'act_std_{i}'].append(acts_arr.std() if acts_arr.size else np.nan)\n",
    "            grads_arr = np.concatenate([g.reshape(-1) for g in layer_grads[i]]) if layer_grads[i] else np.array([])\n",
    "            history[f'grad_norm_{i}'].append(np.linalg.norm(grads_arr))\n",
    "\n",
    "        print(f\"[{init_type}|{activation}|{num_layers}L] \"\n",
    "              f\"Ep{ep}/{config['num_epochs']} \"\n",
    "              f\"TL={tl:.4f} TA={ta:.4f} VL={vl:.4f} VA={va:.4f} T={duration:.2f}s\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(history)\n",
    "    os.makedirs(config['save_dir'], exist_ok=True)\n",
    "    df.to_csv(os.path.join(config['save_dir'], f\"{prefix}.csv\"), index=False)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ce1ed",
   "metadata": {},
   "source": [
    "### Main routine\n",
    "> ⚠️ **Warning: Full Experiment Time**\n",
    ">\n",
    "> Running the complete set of experiments — covering **4 activation functions × 8 initialization strategies × multiple network depths** — may take up to **8 hours** on a GPU.\n",
    ">\n",
    "> For quick testing or debugging, please modify the parameters at the top of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main routine\n",
    "def main():\n",
    "    make_plot_dir()\n",
    "    total_exps = len(CONFIG['init_types']) * len(CONFIG['activations']) * len(CONFIG['layers_list'])\n",
    "    completed = 0\n",
    "    overall_start = time.time()\n",
    "\n",
    "    for init in CONFIG['init_types']:\n",
    "        for act in CONFIG['activations']:\n",
    "            for layer_count in CONFIG['layers_list']:\n",
    "                run_experiment(init, layer_count, act, CONFIG)\n",
    "                completed += 1\n",
    "                elapsed = time.time() - overall_start\n",
    "                avg_per = elapsed / completed\n",
    "                remaining = avg_per * (total_exps - completed)\n",
    "                print(f\"[{completed}/{total_exps}] Elapsed={time.strftime('%H:%M:%S', time.gmtime(elapsed))} \"\n",
    "                      f\"ETA={time.strftime('%H:%M:%S', time.gmtime(remaining))}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205520f8",
   "metadata": {},
   "source": [
    "### Compare different initialization strategies under the same activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different initialization strategies under the same activation function\n",
    "LAYER_LIST = CONFIG[\"layers_list\"]\n",
    "INIT_TYPES = CONFIG[\"init_types\"]\n",
    "ACTIVATIONS = CONFIG[\"activations\"]\n",
    "DATA_DIR = CONFIG[\"save_dir\"]\n",
    "PERCENT_OF_PEAK = 0.95  # e.g., 0.95 means 95% of the peak accuracy\n",
    "\n",
    "# Iterate through each activation function\n",
    "for act in ACTIVATIONS:\n",
    "    init_acc = {init: [] for init in INIT_TYPES}\n",
    "    init_peak_epochs = {init: [] for init in INIT_TYPES}\n",
    "    valid_layers = []\n",
    "\n",
    "    # Load data for all valid initialization types and layer depths\n",
    "    for nl in LAYER_LIST:\n",
    "        all_exist = True\n",
    "        for init in INIT_TYPES:\n",
    "            path = f'{DATA_DIR}/{init}_{act}_{nl}L.csv'\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"⚠️ Missing: {path}\")\n",
    "                all_exist = False\n",
    "                break\n",
    "        if not all_exist:\n",
    "            continue\n",
    "\n",
    "        valid_layers.append(nl)\n",
    "        for init in INIT_TYPES:\n",
    "            df = pd.read_csv(f'{DATA_DIR}/{init}_{act}_{nl}L.csv')\n",
    "            init_acc[init].append(df['train_acc'].iloc[-1])\n",
    "            peak = df['train_acc'].max()\n",
    "            threshold = peak * PERCENT_OF_PEAK\n",
    "            try:\n",
    "                epoch = df[df['train_acc'] >= threshold].index[0] + 1\n",
    "            except IndexError:\n",
    "                epoch = None\n",
    "            init_peak_epochs[init].append(epoch)\n",
    "\n",
    "    # Plot final training accuracy vs number of hidden layers\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for init in INIT_TYPES:\n",
    "        if len(init_acc[init]) == len(valid_layers):\n",
    "            marker = {'he': 'o', 'xavier': 's', 'normal': 'v'}.get(init.split('_')[0], '.')\n",
    "            label = f\"{init} Init\"\n",
    "            plt.plot(valid_layers, init_acc[init], marker=marker, label=label)\n",
    "    plt.title(f'Final Training Accuracy vs. Hidden Layers\\n(Activation: {act})')\n",
    "    plt.xlabel('Number of Hidden Layers')\n",
    "    plt.ylabel('Final Training Accuracy')\n",
    "    plt.xticks(valid_layers)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{DATA_DIR}/final_acc_vs_layers_{act}.png\")\n",
    "    print(f\"✅ Saved plot: {DATA_DIR}/final_acc_vs_layers_{act}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot epochs needed to reach P% of peak accuracy\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for init in INIT_TYPES:\n",
    "        if len(init_peak_epochs[init]) == len(valid_layers):\n",
    "            marker = {'he': 'o', 'xavier': 's', 'normal': 'v'}.get(init.split('_')[0], '.')\n",
    "            label = f\"{init} Init ({int(PERCENT_OF_PEAK * 100)}% Peak)\"\n",
    "            plt.plot(valid_layers, init_peak_epochs[init], marker=marker, label=label)\n",
    "    plt.title(f'Epochs to Reach {int(PERCENT_OF_PEAK * 100)}% Peak Accuracy\\n(Activation: {act})')\n",
    "    plt.xlabel('Number of Hidden Layers')\n",
    "    plt.ylabel('Epochs')\n",
    "    plt.xticks(valid_layers)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{DATA_DIR}/epochs_to_peak_{act}.png\")\n",
    "    print(f\"✅ Saved plot: {DATA_DIR}/epochs_to_peak_{act}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80e357",
   "metadata": {},
   "source": [
    "### Compare different activation functions under the same initialization strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different activation functions under the same initialization strategy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Automatically read parameters from CONFIG\n",
    "LAYER_LIST = CONFIG[\"layers_list\"]\n",
    "INIT_TYPES = CONFIG[\"init_types\"]\n",
    "ACTIVATIONS = CONFIG[\"activations\"]\n",
    "DATA_DIR = CONFIG[\"save_dir\"]\n",
    "PERCENT_OF_PEAK = 0.95  # e.g., 0.95 means 95% of the peak training accuracy\n",
    "\n",
    "# Iterate through each initialization strategy\n",
    "for init in INIT_TYPES:\n",
    "    act_acc = {act: [] for act in ACTIVATIONS}\n",
    "    act_epochs = {act: [] for act in ACTIVATIONS}\n",
    "    valid_layers = []\n",
    "\n",
    "    # Check if all data exists for each activation under current init type\n",
    "    for nl in LAYER_LIST:\n",
    "        all_exist = True\n",
    "        for act in ACTIVATIONS:\n",
    "            path = f'{DATA_DIR}/{init}_{act}_{nl}L.csv'\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"⚠️ Missing: {path}\")\n",
    "                all_exist = False\n",
    "                break\n",
    "        if not all_exist:\n",
    "            continue\n",
    "\n",
    "        valid_layers.append(nl)\n",
    "        for act in ACTIVATIONS:\n",
    "            df = pd.read_csv(f'{DATA_DIR}/{init}_{act}_{nl}L.csv')\n",
    "            act_acc[act].append(df['train_acc'].iloc[-1])\n",
    "            peak = df['train_acc'].max()\n",
    "            threshold = peak * PERCENT_OF_PEAK\n",
    "            try:\n",
    "                epoch = df[df['train_acc'] >= threshold].index[0] + 1\n",
    "            except IndexError:\n",
    "                epoch = None\n",
    "            act_epochs[act].append(epoch)\n",
    "\n",
    "    # Plot final training accuracy vs number of hidden layers\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for act in ACTIVATIONS:\n",
    "        if len(act_acc[act]) == len(valid_layers):\n",
    "            plt.plot(valid_layers, act_acc[act], marker='o', label=act)\n",
    "    plt.title(f\"Final Training Accuracy vs Hidden Layers\\n(Init: {init})\")\n",
    "    plt.xlabel(\"Number of Hidden Layers\")\n",
    "    plt.ylabel(\"Final Training Accuracy\")\n",
    "    plt.xticks(valid_layers)\n",
    "    plt.legend(title=\"Activation\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    save_path1 = f\"{DATA_DIR}/acc_vs_layers_{init}.png\"\n",
    "    plt.savefig(save_path1)\n",
    "    print(f\"✅ Saved plot: {save_path1}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot epochs required to reach X% of peak training accuracy\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for act in ACTIVATIONS:\n",
    "        if len(act_epochs[act]) == len(valid_layers):\n",
    "            plt.plot(valid_layers, act_epochs[act], marker='s', label=act)\n",
    "    plt.title(f\"Epochs to Reach {int(PERCENT_OF_PEAK*100)}% Peak Accuracy\\n(Init: {init})\")\n",
    "    plt.xlabel(\"Number of Hidden Layers\")\n",
    "    plt.ylabel(\"Epochs\")\n",
    "    plt.xticks(valid_layers)\n",
    "    plt.legend(title=\"Activation\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    save_path2 = f\"{DATA_DIR}/epoch_vs_layers_{init}.png\"\n",
    "    plt.savefig(save_path2)\n",
    "    print(f\"✅ Saved plot: {save_path2}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ff921",
   "metadata": {},
   "source": [
    "### Activation Mean and Std Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Mean and Std Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ✅ Manually specify parameters\n",
    "DATA_DIR = \"results\"         # Directory where your CSV files are stored\n",
    "init_type = \"orthogonal\"     # Initialization method\n",
    "activation = \"relu\"          # Activation function\n",
    "layer_list = [40, 30]        # Target layer counts\n",
    "\n",
    "for num_layers in layer_list:\n",
    "    csv_path = os.path.join(DATA_DIR, f\"{init_type}_{activation}_{num_layers}L.csv\")\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"File not found: {csv_path}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Automatically find all activation mean and std columns\n",
    "    act_mean_cols = [col for col in df.columns if col.startswith(\"act_mean_\")]\n",
    "    act_std_cols = [col for col in df.columns if col.startswith(\"act_std_\")]\n",
    "    epochs = range(1, len(df) + 1)\n",
    "\n",
    "    # Plot Activation Mean\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in act_mean_cols:\n",
    "        layer_idx = col.split(\"_\")[-1]\n",
    "        plt.plot(epochs, df[col], label=f\"Layer {layer_idx}\")\n",
    "    plt.title(f\"Activation Mean vs Epochs\\n({init_type}, {activation}, {num_layers} Layers)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Activation Mean\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Activation Std\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in act_std_cols:\n",
    "        layer_idx = col.split(\"_\")[-1]\n",
    "        plt.plot(epochs, df[col], label=f\"Layer {layer_idx}\")\n",
    "    plt.title(f\"Activation Std vs Epochs\\n({init_type}, {activation}, {num_layers} Layers)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Activation Std\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e57bf",
   "metadata": {},
   "source": [
    "### Gradient Norm Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac47e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Norm Visualization\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "DATA_DIR = \"results\"\n",
    "init = \"orthogonal\"\n",
    "activation = \"relu\"\n",
    "num_layers = 50\n",
    "csv_path = os.path.join(DATA_DIR, f\"{init}_{activation}_{num_layers}L.csv\")\n",
    "target_epochs = [0, 9, 19, 49]  # Note: Index starts from 0\n",
    "\n",
    "# Load CSV data\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"File not found: {csv_path}\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Automatically identify all grad_norm_* columns\n",
    "grad_cols = [col for col in df.columns if col.startswith(\"grad_norm_\")]\n",
    "layer_indices = [int(col.split(\"_\")[-1]) for col in grad_cols]\n",
    "layer_indices.sort()\n",
    "\n",
    "# Plot gradient norm for selected epochs\n",
    "for ep in target_epochs:\n",
    "    if ep >= len(df):\n",
    "        print(f\"⚠️ Epoch {ep} exceeds CSV length, skipping\")\n",
    "        continue\n",
    "\n",
    "    grads = [df.loc[ep, f\"grad_norm_{i}\"] for i in layer_indices]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(layer_indices, grads, marker='o')\n",
    "    plt.title(f\"Gradient Norm per Layer at Epoch {ep}\\n({activation}, {init}, {num_layers}L)\")\n",
    "    plt.xlabel(\"Layer Index\")\n",
    "    plt.ylabel(\"Gradient Norm (L2)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e51bd",
   "metadata": {},
   "source": [
    "### Accuracy Generalization Comparison Across Different Layer Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Generalization Comparison Across Different Layer Depths\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "DATA_DIR = \"results\"\n",
    "init_types = [\"orthogonal\", \"xavier_normal\"]\n",
    "activations = [\"relu\"]  # Optionally include: \"sigmoid\", \"tanh\", \"gelu\"\n",
    "layers_list = [5, 10, 20, 30, 40, 50]\n",
    "\n",
    "# Main function\n",
    "def plot_by_init_separately():\n",
    "    for init in init_types:\n",
    "        plt.figure(figsize=(9, 5))\n",
    "        for act in activations:\n",
    "            train_accs, test_accs, valid_layers = [], [], []\n",
    "            for nl in layers_list:\n",
    "                path = os.path.join(DATA_DIR, f\"{init}_{act}_{nl}L.csv\")\n",
    "                if not os.path.exists(path):\n",
    "                    print(f\"⚠️ Missing file: {path}\")\n",
    "                    continue\n",
    "                df = pd.read_csv(path)\n",
    "                if 'train_acc' not in df.columns or 'test_acc' not in df.columns:\n",
    "                    print(f\"⚠️ Missing train/test_acc columns in: {path}\")\n",
    "                    continue\n",
    "                train_accs.append(df['train_acc'].iloc[-1])\n",
    "                test_accs.append(df['test_acc'].iloc[-1])\n",
    "                valid_layers.append(nl)\n",
    "            # Plot for each activation function\n",
    "            if train_accs:\n",
    "                plt.plot(valid_layers, train_accs, marker='o', linestyle='--', label=f\"{act} - Train\")\n",
    "                plt.plot(valid_layers, test_accs, marker='s', linestyle='-', label=f\"{act} - Test\")\n",
    "\n",
    "        # Plot style\n",
    "        plt.title(f\"Final Accuracy vs Hidden Layers\\nInit: {init}\")\n",
    "        plt.xlabel(\"Number of Hidden Layers\")\n",
    "        plt.ylabel(\"Accuracy at Final Epoch\")\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.xticks(layers_list)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run\n",
    "plot_by_init_separately()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5dd37a",
   "metadata": {},
   "source": [
    "## 📎 Appendix: NumPy Implementation (Using `courselib`)\n",
    "\n",
    "The following is the NumPy-based implementation integrated with `courselib`,  \n",
    "kept for verification and educational reference. This version is not used in the full-scale experiments due to its slower runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepmlp_numpy.py\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from courselib.models.nn import DenseLayer, ReLU, Sigmoid, Linear, Softmax, CrossEntropy\n",
    "from courselib.optimizers import GDOptimizer\n",
    "from courselib.models.base import TrainableModel\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# ==================== Configuration Parameters ====================\n",
    "CONFIG = {\n",
    "    \"layers_list\": [5, 10, 20, 30, 40, 50],\n",
    "    \"init_types\": [\n",
    "        \"he_normal\", \"he_uniform\",\n",
    "        \"xavier_normal\", \"xavier_uniform\",\n",
    "        \"normal\", \"zero\",\n",
    "        \"orthogonal\", \"trunc_normal\"\n",
    "    ],\n",
    "    \"activations\": [\"ReLU\", \"Sigmoid\"],\n",
    "    \"init_params\": {\n",
    "        \"he_normal\":   {\"factor\": 2.0, \"mode\": \"fan_in\"},\n",
    "        \"he_uniform\":  {\"factor\": 2.0, \"mode\": \"fan_in\"},\n",
    "        \"orthogonal\":  {\"gain\": 1.0},\n",
    "        \"trunc_normal\": {\"mean\": 0.0, \"std\": 0.01, \"a\": -2.0, \"b\": 2.0}\n",
    "    },\n",
    "    \"hidden_size\": 128,\n",
    "    \"input_size\": 784,\n",
    "    \"output_size\": 10,\n",
    "    \"num_epochs\": 50,\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"save_dir\": \"results_numpy\"\n",
    "}\n",
    "\n",
    "# ==================== Weight Initialization ====================\n",
    "def initialize_weights(model, init_type: str, params: Dict):\n",
    "    \"\"\"\n",
    "    Initialize model parameters according to the specified initialization type and parameters.\n",
    "    \"\"\"\n",
    "    for key, value in model.params.items():\n",
    "        if key.endswith('_W'):\n",
    "            shape = value.shape\n",
    "            if init_type == \"he_normal\":\n",
    "                fan_in = shape[0]\n",
    "                std = np.sqrt(params.get(\"factor\", 2.0) / fan_in)\n",
    "                model.params[key] = np.random.randn(*shape) * std\n",
    "            elif init_type == \"he_uniform\":\n",
    "                fan_in = shape[0]\n",
    "                bound = np.sqrt(3.0 * params.get(\"factor\", 2.0) / fan_in)\n",
    "                model.params[key] = np.random.uniform(-bound, bound, size=shape)\n",
    "            elif init_type == \"xavier_normal\":\n",
    "                fan_in, fan_out = shape\n",
    "                std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "                model.params[key] = np.random.randn(*shape) * std\n",
    "            elif init_type == \"xavier_uniform\":\n",
    "                fan_in, fan_out = shape\n",
    "                bound = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "                model.params[key] = np.random.uniform(-bound, bound, size=shape)\n",
    "            elif init_type == \"normal\":\n",
    "                mean = params.get(\"mean\", 0.0)\n",
    "                std = params.get(\"std\", 0.01)\n",
    "                model.params[key] = np.random.normal(mean, std, size=shape)\n",
    "            elif init_type == \"zero\":\n",
    "                model.params[key] = np.zeros(shape)\n",
    "        elif key.endswith('_b'):\n",
    "            model.params[key] = np.zeros_like(value)\n",
    "\n",
    "# ==================== Data Loading ====================\n",
    "def get_dataloaders(batch_size: int):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    def to_numpy_loader(dataset):\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        for x, y in loader:\n",
    "            yield x.numpy(), np.eye(CONFIG['output_size'])[y.numpy()]\n",
    "\n",
    "    return to_numpy_loader(train_dataset), to_numpy_loader(test_dataset)\n",
    "\n",
    "# ==================== Model Definition ====================\n",
    "class DeepMLP(TrainableModel):\n",
    "    def __init__(\n",
    "        self, input_size: int, hidden_size: int, output_size: int,\n",
    "        num_hidden_layers: int, activation: str, optimizer\n",
    "    ):\n",
    "        super().__init__(optimizer)\n",
    "        self.layers = []\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        self._inputs = []  # Cache inputs for each layer\n",
    "        self._zs = []      # Cache pre-activations for each layer\n",
    "\n",
    "        # Construct hidden layers\n",
    "        for i in range(num_hidden_layers):\n",
    "            in_size = input_size if i == 0 else hidden_size\n",
    "            layer = DenseLayer(in_size, hidden_size, activation=activation, layer_name=f\"L{i}\")\n",
    "            self.layers.append(layer)\n",
    "            for k, v in layer._get_params().items():\n",
    "                self.params[k] = v\n",
    "                self.grads[k] = np.zeros_like(v)\n",
    "\n",
    "        # Construct output layer (Linear + Softmax)\n",
    "        out_layer = DenseLayer(hidden_size, output_size, activation='Linear', layer_name=\"Out\")\n",
    "        self.layers.append(out_layer)\n",
    "        for k, v in out_layer._get_params().items():\n",
    "            self.params[k] = v\n",
    "            self.grads[k] = np.zeros_like(v)\n",
    "\n",
    "        self.loss_fn = CrossEntropy()\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, X: np.ndarray, training: bool=False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform forward propagation through all layers and return softmax output.\n",
    "        \"\"\"\n",
    "        self._inputs = []\n",
    "        self._zs = []\n",
    "        out = X\n",
    "        for layer in self.layers:\n",
    "            self._inputs.append(out)\n",
    "            z, out = layer(out)\n",
    "            self._zs.append(z)\n",
    "        return self.softmax(out)\n",
    "\n",
    "    def loss(self, Y_pred: np.ndarray, Y_true: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the loss between predictions and true labels.\n",
    "        \"\"\"\n",
    "        return self.loss_fn(Y_pred, Y_true)\n",
    "\n",
    "    def backward(self, X: np.ndarray, Y_pred: np.ndarray, Y_true: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform backward propagation: compute gradients for each layer.\n",
    "        \"\"\"\n",
    "        delta = self.loss_fn.grad(Y_pred, Y_true)  # Initial gradient from softmax+crossentropy\n",
    "        for idx in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[idx]\n",
    "            prev_out = self._inputs[idx]\n",
    "            grads = layer.loss_grad(prev_out, delta)\n",
    "            for k, g in grads.items():\n",
    "                self.grads[k] = g\n",
    "            if idx > 0:\n",
    "                W_next = layer.W\n",
    "                z_prev = self._zs[idx-1]\n",
    "                delta = self.layers[idx-1].compute_delta(z_prev, W_next, delta)\n",
    "\n",
    "# ==================== Training and Evaluation ====================\n",
    "def train(model: DeepMLP, optimizer: GDOptimizer, loader) -> tuple:\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        y_pred = model.forward(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "        model.backward(x, y_pred, y)\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss * x.shape[0]\n",
    "        correct += (np.argmax(y_pred, axis=1) == np.argmax(y, axis=1)).sum()\n",
    "        total += x.shape[0]\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def evaluate(model: DeepMLP, loader) -> tuple:\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        y_pred = model.forward(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "        total_loss += loss * x.shape[0]\n",
    "        correct += (np.argmax(y_pred, axis=1) == np.argmax(y, axis=1)).sum()\n",
    "        total += x.shape[0]\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# ==================== Experiment Workflow ====================\n",
    "def run_experiment(init_type: str, num_layers: int, activation: str, config: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Run a single experiment: initialize model, train, evaluate, and record stats.\n",
    "    \"\"\"\n",
    "    train_loader, test_loader = get_dataloaders(config['batch_size'])\n",
    "    optimizer = GDOptimizer(learning_rate=config['lr'])\n",
    "    model = DeepMLP(\n",
    "        config['input_size'], config['hidden_size'], config['output_size'],\n",
    "        num_layers, activation, optimizer\n",
    "    )\n",
    "    initialize_weights(model, init_type, config.get('init_params', {}).get(init_type, {}))\n",
    "\n",
    "    # Prepare history dictionary with basic metrics and per-layer stats\n",
    "    history = {\n",
    "        'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': [], 'epoch_time': []\n",
    "    }\n",
    "    num_hidden = len(model.layers) - 1\n",
    "    for i in range(num_hidden):\n",
    "        history[f'act_mean_{i}'] = []\n",
    "        history[f'act_std_{i}']  = []\n",
    "        history[f'grad_norm_{i}'] = []\n",
    "\n",
    "    prefix = f\"{init_type}_{activation}_{num_layers}L\"\n",
    "    os.makedirs(config['save_dir'], exist_ok=True)\n",
    "\n",
    "    for ep in range(1, config['num_epochs'] + 1):\n",
    "        # Reinitialize data loaders each epoch to avoid exhaustion\n",
    "        train_loader, test_loader = get_dataloaders(config['batch_size'])\n",
    "\n",
    "        start = time.time()\n",
    "        tl, ta = train(model, optimizer, train_loader)\n",
    "        vl, va = evaluate(model, test_loader)\n",
    "        duration = time.time() - start\n",
    "\n",
    "        # Record basic metrics\n",
    "        history['train_loss'].append(tl)\n",
    "        history['test_loss'].append(vl)\n",
    "        history['train_acc'].append(ta)\n",
    "        history['test_acc'].append(va)\n",
    "        history['epoch_time'].append(duration)\n",
    "\n",
    "        # Record per-layer activation and gradient statistics\n",
    "        for i in range(num_hidden):\n",
    "            z = model._zs[i]\n",
    "            history[f'act_mean_{i}'].append(z.mean())\n",
    "            history[f'act_std_{i}'].append(z.std())\n",
    "            grad = model.grads[f\"L{i}_W\"]\n",
    "            history[f'grad_norm_{i}'].append(np.linalg.norm(grad))\n",
    "\n",
    "        print(f\"[{init_type}|{activation}|{num_layers}L] \"\n",
    "              f\"Epoch {ep}/{config['num_epochs']} \"\n",
    "              f\"TrainLoss={tl:.4f} TrainAcc={ta:.4f} \"\n",
    "              f\"TestLoss={vl:.4f} TestAcc={va:.4f} \"\n",
    "              f\"Time={duration:.2f}s\")\n",
    "\n",
    "    # Save history to CSV\n",
    "    df = pd.DataFrame(history)\n",
    "    df.to_csv(os.path.join(config['save_dir'], f\"{prefix}.csv\"), index=False)\n",
    "    return history\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "    total_exps = len(CONFIG['init_types']) * len(CONFIG['activations']) * len(CONFIG['layers_list'])\n",
    "    completed = 0\n",
    "    overall_start = time.time()\n",
    "\n",
    "    for init in CONFIG['init_types']:\n",
    "        for act in CONFIG['activations']:\n",
    "            for layers in CONFIG['layers_list']:\n",
    "                run_experiment(init, layers, act, CONFIG)\n",
    "                completed += 1\n",
    "                elapsed = time.time() - overall_start\n",
    "                avg = elapsed / completed\n",
    "                eta = avg * (total_exps - completed)\n",
    "                print(f\"[{completed}/{total_exps}] Elapsed={time.strftime('%H:%M:%S', time.gmtime(elapsed))} \"\n",
    "                      f\"ETA={time.strftime('%H:%M:%S', time.gmtime(eta))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
